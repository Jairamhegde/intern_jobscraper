
from scraper.fetcher import get_soup
from scraper.extractor import scrape_data
from scraper.cleanData import clean_data
from insights.insight import generate_insights
from app import DashBoard
from pathlib import Path
import subprocess
import sys
import logging
import sqlite3
from db.db_manager import manage_operation



logging.basicConfig(
     filename="logfile.log",
     level=logging.INFO,
     format='%(asctime)s-%(levelname)s-%(name)s-%(message)s'
)
logging.info("Execution started..")
def internshala(url,pages):    
        newl=[]
        for i in range(1,pages+1):
            if i==1:
                link=url
            else:
                link=url+f'page-{i}'
            g=get_soup(link)
            x=scrape_data(g)
            logging.info(f"Scraped data from page {i}")
            manage_operation(x)
         

def run_streamlit():
    subprocess.run([
        sys.executable, "-m", "streamlit", "run", "app.py"
    ])
     
if __name__=='__main__':
    internshala("https://internshala.com/jobs/ai-agent-development,backend-development-jobs/",3)   
    # run_streamlit()
